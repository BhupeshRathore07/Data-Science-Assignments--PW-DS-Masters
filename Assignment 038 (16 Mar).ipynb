{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7f2701-2f38-4eaa-8883-abd88c414138",
   "metadata": {},
   "source": [
    "#### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7f8f7-b645-48d3-9687-cb1cc9e0d203",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1. Overfitting occurs when a model learns the training data too well, to the point where it memorizes it instead of generalizing. This results in a model that performs well on the training data but poorly on new, unseen data.\n",
    "    - The consequences of overfitting are that the model will have poor performance on new data and will not be able to make accurate predictions.\n",
    "\n",
    "2. Underfitting occurs when a model is too simple and fails to capture the complexity of the data. This results in a model that performs poorly on both the training data and new data. \n",
    "    - The consequences of underfitting are that the model will not be able to make accurate predictions and it may miss important patterns in the data.\n",
    "\n",
    "\n",
    "- To mitigate overfitting: Regularization, Cross-validation\n",
    "- To mitigate underfitting: Increasing the complexity of the model, Feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c32bc7-a72b-467d-bdce-7842e4c29457",
   "metadata": {},
   "source": [
    "#### Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecc291-c69c-4e3e-89ef-0027d050ee62",
   "metadata": {},
   "source": [
    "Ans: Ways to reduce overfitting: \n",
    "1. Regularization: It is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance. \n",
    "\n",
    "2. Cross-validation: It is a technique where the dataset is divided into training and validation sets, and evaluated on the validation set. This helps to detect overfitting by measuring the performance of the model on new data.\n",
    "\n",
    "3. Early stopping: It is a technique where the model is trained for a certain number of epochs and then stopped before it starts to overfit.\n",
    "\n",
    "4. Dropout: It is a regularization technique that randomly drops out some of the neurons in a neural network during training.\n",
    "\n",
    "5. Data augmentation: It is a technique where the training data is artificially expanded by applying transformations, such as rotation or scaling, to the original data. This helps to increase the size and diversity of the training data, which can improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3dcdb-959e-48b9-815e-623695f8e07f",
   "metadata": {},
   "source": [
    "#### Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14136a55-5437-4782-8876-cccbe3a67c0d",
   "metadata": {},
   "source": [
    "Ans: **Underfitting** occurs when a model is too simple to capture the complexity of the underlying patterns in the data, resulting in poor performance on both the training and test sets.\n",
    "\n",
    "Scenarios where underfitting can occur in ML:\n",
    "1. Insufficient model complexity: If the model is too simple to capture the underlying patterns in the data, it may result in underfitting. For example, using a linear regression model to fit a non-linear relationship between the features and the target variable can result in underfitting.\n",
    "\n",
    "2. Insufficient training data: If the size of the training dataset is too small, the model may not be able to capture the underlying patterns in the data, resulting in underfitting.\n",
    "\n",
    "3. Over-regularization: While regularization can help to prevent overfitting, too much regularization can lead to underfitting.\n",
    "\n",
    "4. Feature selection: If important features are not included in the model, it may result in underfitting.\n",
    "\n",
    "5. Noisy data: If the training data contains noise, such as measurement errors or outliers, it may result in underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4bb36c-8945-47b4-8f33-aac21f9fe2dc",
   "metadata": {},
   "source": [
    "#### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d132863-d510-48fe-bb8c-7c7f959bf847",
   "metadata": {},
   "source": [
    "Ans: The bias-variance tradeoff in machine learning is the relationship between a model's ability to fit the training data and its ability to generalize to new, unseen data. \n",
    "\n",
    "Bias refers to a model's underfitting and variance refers to its overfitting. \n",
    "\n",
    "The ideal model achieves a balance between bias and variance for good performance on both the training and test sets. \n",
    "\n",
    "Increasing complexity reduces bias but increases variance, while reducing complexity reduces variance but increases bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a484c-4c43-4fda-8525-191b08d106e8",
   "metadata": {},
   "source": [
    "#### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9988458-ee70-440f-85bd-2e8e1cf090dc",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1. Train-test split: Splitting the data into training and testing sets and evaluating the model's performance on both sets. If the model performs well on the training set but poorly on the testing set, it is likely overfitting. If the model performs poorly on both sets, it is likely underfitting.\n",
    "\n",
    "2. Cross-validation: Dividing the data into multiple folds and evaluating the model's performance on each fold. If the model performs well on some folds but poorly on others, it is likely overfitting. If the model performs poorly on all folds, it is likely underfitting.\n",
    "\n",
    "3. Learning curve: Plotting the model's performance on the training and testing sets as a function of the number of training examples. If the model's performance on the training set is much better than its performance on the testing set, it is likely overfitting. If both performances are poor and converge, it is likely underfitting.\n",
    "\n",
    "4. Regularization: Adding penalties to the model's loss function to discourage overfitting. If the regularization parameter is too high, the model may underfit. If it is too low, the model may overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5170aa8-cd2e-4fb3-8a8b-4992e243c443",
   "metadata": {},
   "source": [
    "#### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f77f08-3be3-4ec5-a9d6-2efab33f7654",
   "metadata": {},
   "source": [
    "Ans:\n",
    "- Bias is the error between a model's predictions and the true values, while variance is the sensitivity of the model's predictions to noise in the training data. \n",
    "\n",
    "- High bias models are too simple and underfit the data, while high variance models are too complex and overfit the data. \n",
    "\n",
    "- High bias models have high error on both training and testing data, while high variance models have low training error but high testing error. \n",
    "\n",
    "- Examples of high bias models include linear regression models that try to fit a nonlinear relationship and decision trees with limited depth.\n",
    "\n",
    "- Examples of high variance models include decision trees with large depth and high degree polynomial regression models.\n",
    "\n",
    "- High bias models tend to have poor performance on both the training and testing data, while high variance models tend to have good performance on the training data but poor performance on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a07b6b-6673-4dfd-af11-6768fd7d1205",
   "metadata": {},
   "source": [
    "#### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6058fd2-22f3-46b9-b9c3-11f549d11970",
   "metadata": {},
   "source": [
    "Ans: **Regularization** is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. This penalty term encourages the model to have smaller weights, thereby reducing its complexity and preventing it from fitting the noise in the training data.\n",
    "\n",
    "Common regularization techniques include\n",
    "- L1 regularization (Lasso) (L1 regularization adds the absolute values of the weights to the loss function)\n",
    "- L2 regularization (Ridge) (L2 regularization adds the squared values of the weights)  \n",
    "- Elastic Net regularization (Elastic Net regularization combines both L1 and L2 regularization)\n",
    "\n",
    "Regularization techniques work by shrinking the model's weights towards zero, thereby reducing its complexity and preventing overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
